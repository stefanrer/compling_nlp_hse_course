{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dba7c0d",
   "metadata": {},
   "source": [
    "# Домашнее задание № 11. Машинный перевод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Yj7aripVIsbG",
   "metadata": {
    "id": "Yj7aripVIsbG"
   },
   "source": [
    "## Задание 1 (6 баллов + 2 доп балла).\n",
    "Нужно обучить трансформер на этом же или на другом корпусе (можно взять другую языковую пару с того же сайте) и оценивать его на всей тестовой выборке (а не на 10 примерах как сделал я). \n",
    "\n",
    "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Подсказка: модель может предсказывать батчами.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d202c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T20:47:31.743927400Z",
     "start_time": "2023-06-16T20:47:27.704050600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
    "text = open('opus.en-ru-train.ru.txt', encoding=\"UTF8\").read().replace('\\xa0', ' ')\n",
    "f = open('opus.en-ru-train.ru.txt', 'w', encoding=\"UTF8\")\n",
    "f.write(text)\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T21:17:30.289405700Z",
     "start_time": "2023-06-16T21:17:29.083713100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "text = open('opus.en-ru-test.ru.txt', encoding=\"UTF8\").read().replace('\\xa0', ' ')\n",
    "f = open('opus.en-ru-test.ru.txt', 'w', encoding=\"UTF8\")\n",
    "f.write(text)\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T21:17:48.081039800Z",
     "start_time": "2023-06-16T21:17:48.066522700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "en_sents = open('opus.en-ru-train.en.txt', encoding=\"UTF8\").read().splitlines()\n",
    "ru_sents = open('opus.en-ru-train.ru.txt', encoding=\"UTF8\").read().splitlines()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T23:14:29.785366600Z",
     "start_time": "2023-06-16T23:14:27.664632900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "('So what are you thinking?', 'Ну и что ты думаешь?')"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sents[-1], ru_sents[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T23:14:29.802624200Z",
     "start_time": "2023-06-16T23:14:29.787637900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(BPE())\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "trainer_en = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer_en.train(files=[\"opus.en-ru-train.en.txt\"], trainer=trainer_en)\n",
    "\n",
    "tokenizer_ru = Tokenizer(BPE())\n",
    "tokenizer_ru.pre_tokenizer = Whitespace()\n",
    "trainer_ru = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer_ru.train(files=[\"opus.en-ru-train.ru.txt\"], trainer=trainer_ru)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T23:15:09.687018100Z",
     "start_time": "2023-06-16T23:14:57.069167200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# раскоментируйте эту ячейку при обучении токенизатора\n",
    "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
    "# tokenizer_en.save('tokenizer_en')\n",
    "# tokenizer_ru.save('tokenizer_ru')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T23:15:15.229156300Z",
     "start_time": "2023-06-16T23:15:15.214919200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
    "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T23:15:26.343983300Z",
     "start_time": "2023-06-16T23:15:26.327736200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, max_len):\n",
    "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[SEP]')]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T21:25:58.741052800Z",
     "start_time": "2023-06-16T21:25:58.724532500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
    "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
    "PAD_IDX"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T23:15:37.200091900Z",
     "start_time": "2023-06-16T23:15:37.182531200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# ограничимся длинной в 30 и 35 (разные чтобы показать что в seq2seq не нужна одинаковая длина)\n",
    "max_len_en, max_len_ru = 30, 35"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T21:34:32.388189300Z",
     "start_time": "2023-06-16T21:34:32.378328900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "X_en = [encode(t, tokenizer_en, max_len_en) for t in en_sents]\n",
    "X_ru = [encode(t, tokenizer_ru, max_len_ru) for t in ru_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T23:16:38.688988900Z",
     "start_time": "2023-06-16T23:15:44.141943100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "(1000000, 1000000)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# миллион примеров\n",
    "X_en.__len__(), X_ru.__len__()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T23:16:38.700576500Z",
     "start_time": "2023-06-16T23:16:38.686291800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, texts_en, texts_ru):\n",
    "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
    "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, padding_value=PAD_IDX)\n",
    "\n",
    "        self.texts_ru = [torch.LongTensor(sent) for sent in texts_ru]\n",
    "        self.texts_ru = torch.nn.utils.rnn.pad_sequence(self.texts_ru, padding_value=PAD_IDX)\n",
    "\n",
    "        self.length = len(texts_en)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        ids_en = self.texts_en[:, index]\n",
    "        ids_ru = self.texts_ru[:, index]\n",
    "\n",
    "        return ids_en, ids_ru"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T21:55:07.812407700Z",
     "start_time": "2023-06-16T21:55:07.800600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "X_en_train, X_en_valid, X_ru_train, X_ru_valid = train_test_split(X_en, X_ru, test_size=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T23:16:39.339651700Z",
     "start_time": "2023-06-16T23:16:38.702082300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "training_set = Dataset(X_en_train, X_ru_train)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=200, shuffle=True, )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T23:16:52.789646900Z",
     "start_time": "2023-06-16T23:16:39.342650300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "valid_set = Dataset(X_en_valid, X_ru_valid)\n",
    "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=200, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T23:16:53.382303Z",
     "start_time": "2023-06-16T23:16:52.789646900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 150):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "#         print('pos inp')\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "#         print('pos dec')\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "#         print('pos out')\n",
    "        x = self.generator(outs)\n",
    "#         print('gen')\n",
    "        return x\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n",
    "# During training, we need a subsequent word mask that will prevent model to look into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let’s define a function that will take care of both.\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T23:16:58.926001600Z",
     "start_time": "2023-06-16T23:16:58.906967800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from time import time\n",
    "def train(model, iterator, optimizer, criterion, print_every=500):\n",
    "\n",
    "    epoch_loss = []\n",
    "    ac = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, (texts_en, texts_ru) in enumerate(iterator):\n",
    "        texts_en = texts_en.T.to(DEVICE) # чтобы батч был в конце\n",
    "        texts_ru = texts_ru.T.to(DEVICE) # чтобы батч был в конце\n",
    "\n",
    "        # помимо текста в модель еще нужно передать целевую последовательность\n",
    "        # но не полную а без 1 последнего элемента\n",
    "        # а на выходе ожидаем, что модель сгенерирует этот недостающий элемент\n",
    "        texts_ru_input = texts_ru[:-1, :]\n",
    "\n",
    "\n",
    "        # в трансформерах нет циклов как в лстм\n",
    "        # каждый элемент связан с каждым через аттеншен\n",
    "        # чтобы имитировать последовательную обработку\n",
    "        # и чтобы не считать аттеншн с паддингом\n",
    "        # в трансформерах нужно считать много масок\n",
    "        # подробнее про это по ссылкам выше\n",
    "        (texts_en_mask, texts_ru_mask,\n",
    "        texts_en_padding_mask, texts_ru_padding_mask) = create_mask(texts_en, texts_ru_input)\n",
    "        logits = model(texts_en, texts_ru_input, texts_en_mask, texts_ru_mask,\n",
    "                       texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # сравниваем выход из модели с целевой последовательностью уже с этим последним элементом\n",
    "        texts_ru_out = texts_ru[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_ru_out.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        if not (i+1) % print_every:\n",
    "            print(f'Loss: {np.mean(epoch_loss)};')\n",
    "\n",
    "    return np.mean(epoch_loss)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_f1 = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (texts_en, texts_ru) in enumerate(iterator):\n",
    "            texts_en = texts_en.T.to(DEVICE)\n",
    "            texts_ru = texts_ru.T.to(DEVICE)\n",
    "\n",
    "            texts_ru_input = texts_ru[:-1, :]\n",
    "\n",
    "            (texts_en_mask, texts_ru_mask,\n",
    "            texts_en_padding_mask, texts_ru_padding_mask) = create_mask(texts_en, texts_ru_input)\n",
    "\n",
    "            logits = model(texts_en, texts_ru_input, texts_en_mask, texts_ru_mask,\n",
    "                           texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "\n",
    "\n",
    "            texts_ru_out = texts_ru[1:, :]\n",
    "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_ru_out.reshape(-1))\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(epoch_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T22:16:04.529074300Z",
     "start_time": "2023-06-16T22:16:04.515436200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "EN_VOCAB_SIZE = tokenizer_en.get_vocab_size()\n",
    "RU_VOCAB_SIZE = tokenizer_ru.get_vocab_size()\n",
    "\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "\n",
    "try:\n",
    "    transformer = torch.load('model')\n",
    "except:\n",
    "    transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                     NHEAD, EN_VOCAB_SIZE, RU_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T00:13:00.773925400Z",
     "start_time": "2023-06-17T00:13:00.693128500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T00:13:05.767403600Z",
     "start_time": "2023-06-17T00:13:05.726966400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robst\\anaconda3\\envs\\gpt2\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.060866549491882;\n",
      "Loss: 6.5951112375259395;\n",
      "Loss: 6.300608988444011;\n",
      "Loss: 6.078218590974807;\n",
      "Loss: 5.902960467147827;\n",
      "Loss: 5.756781321684519;\n",
      "Loss: 5.628788885797773;\n",
      "Loss: 5.517271479129791;\n",
      "Loss: 5.416293688774109;\n",
      "First epoch - 4.331290998458862, saving model..\n",
      "Epoch: 1, Train loss: 5.370, Val loss: 4.331,            Epoch time=513.710s\n",
      "Loss: 4.390142631530762;\n",
      "Loss: 4.3579894037246705;\n",
      "Loss: 4.316447576522827;\n",
      "Loss: 4.279523859739304;\n",
      "Loss: 4.244511232757568;\n",
      "Loss: 4.2106361645857495;\n",
      "Loss: 4.177314560413361;\n",
      "Loss: 4.146812884509563;\n",
      "Loss: 4.117848366737365;\n",
      "Improved from 4.331290998458862 to 3.6575626134872437, saving model..\n",
      "Epoch: 2, Train loss: 4.103, Val loss: 3.658,            Epoch time=509.906s\n",
      "Loss: 3.721607120513916;\n",
      "Loss: 3.7105232753753663;\n",
      "Loss: 3.6969984742800395;\n",
      "Loss: 3.684307337641716;\n",
      "Loss: 3.6689805006980896;\n",
      "Loss: 3.6547887053489685;\n",
      "Loss: 3.6404446503094263;\n",
      "Loss: 3.6264306665062906;\n",
      "Loss: 3.613754880110423;\n",
      "Improved from 3.6575626134872437 to 3.322567325592041, saving model..\n",
      "Epoch: 3, Train loss: 3.607, Val loss: 3.323,            Epoch time=473.199s\n",
      "Loss: 3.3775425233840943;\n",
      "Loss: 3.3724344244003297;\n",
      "Loss: 3.3658543977737425;\n",
      "Loss: 3.361744322538376;\n",
      "Loss: 3.355962233543396;\n",
      "Loss: 3.3488060957590737;\n",
      "Loss: 3.3431552537509375;\n",
      "Loss: 3.3380005145072937;\n",
      "Loss: 3.331588357342614;\n",
      "Improved from 3.322567325592041 to 3.1382930021286013, saving model..\n",
      "Epoch: 4, Train loss: 3.328, Val loss: 3.138,            Epoch time=493.558s\n",
      "Loss: 3.1585909457206727;\n",
      "Loss: 3.159384609460831;\n",
      "Loss: 3.1614534101486207;\n",
      "Loss: 3.1622249139547347;\n",
      "Loss: 3.160982441329956;\n",
      "Loss: 3.1580947429339092;\n",
      "Loss: 3.156650076661791;\n",
      "Loss: 3.153892964601517;\n",
      "Loss: 3.1515246418846976;\n",
      "Improved from 3.1382930021286013 to 3.0177836904525757, saving model..\n",
      "Epoch: 5, Train loss: 3.150, Val loss: 3.018,            Epoch time=493.268s\n",
      "Loss: 3.013997376918793;\n",
      "Loss: 3.01647353553772;\n",
      "Loss: 3.0214105038642884;\n",
      "Loss: 3.0231203335523604;\n",
      "Loss: 3.0247127999305725;\n",
      "Loss: 3.0251551455656687;\n",
      "Loss: 3.026386851991926;\n",
      "Loss: 3.025842970430851;\n",
      "Loss: 3.0253952439096237;\n",
      "Improved from 3.0177836904525757 to 2.937667839050293, saving model..\n",
      "Epoch: 6, Train loss: 3.026, Val loss: 2.938,            Epoch time=494.957s\n",
      "Loss: 2.905803563594818;\n",
      "Loss: 2.9158733887672423;\n",
      "Loss: 2.918734538237254;\n",
      "Loss: 2.922910593032837;\n",
      "Loss: 2.925558952045441;\n",
      "Loss: 2.929068348725637;\n",
      "Loss: 2.9309999287469046;\n",
      "Loss: 2.9307900164723395;\n",
      "Loss: 2.9321923081609937;\n",
      "Improved from 2.937667839050293 to 2.8759001989364625, saving model..\n",
      "Epoch: 7, Train loss: 2.932, Val loss: 2.876,            Epoch time=569.873s\n",
      "Loss: 2.827314014911652;\n",
      "Loss: 2.8337673490047455;\n",
      "Loss: 2.8402203205426533;\n",
      "Loss: 2.845207028031349;\n",
      "Loss: 2.8477380078315733;\n",
      "Loss: 2.851276552438736;\n",
      "Loss: 2.854463663033077;\n",
      "Loss: 2.855741794347763;\n",
      "Loss: 2.8581527019606696;\n",
      "Improved from 2.8759001989364625 to 2.826942732810974, saving model..\n",
      "Epoch: 8, Train loss: 2.859, Val loss: 2.827,            Epoch time=564.730s\n",
      "Loss: 2.7566688809394835;\n",
      "Loss: 2.770691380023956;\n",
      "Loss: 2.7784050445556643;\n",
      "Loss: 2.784699344992638;\n",
      "Loss: 2.789679804801941;\n",
      "Loss: 2.791634834369024;\n",
      "Loss: 2.7931153143474035;\n",
      "Loss: 2.795605332970619;\n",
      "Loss: 2.797444569322798;\n",
      "Improved from 2.826942732810974 to 2.7931915884017946, saving model..\n",
      "Epoch: 9, Train loss: 2.799, Val loss: 2.793,            Epoch time=432.429s\n",
      "Loss: 2.7102518887519835;\n",
      "Loss: 2.7196589357852936;\n",
      "Loss: 2.7276217069625854;\n",
      "Loss: 2.7324201649427415;\n",
      "Loss: 2.735340631389618;\n",
      "Loss: 2.739739106575648;\n",
      "Loss: 2.74305071912493;\n",
      "Loss: 2.7463433233499526;\n",
      "Loss: 2.747817458735572;\n",
      "Improved from 2.7931915884017946 to 2.758474910736084, saving model..\n",
      "Epoch: 10, Train loss: 2.749, Val loss: 2.758,            Epoch time=426.226s\n",
      "Loss: 2.6625951409339903;\n",
      "Loss: 2.6716097671985626;\n",
      "Loss: 2.6788112088839213;\n",
      "Loss: 2.6841691267490386;\n",
      "Loss: 2.6888047441482543;\n",
      "Loss: 2.6926235671043397;\n",
      "Loss: 2.6982092890058245;\n",
      "Loss: 2.7013475751280787;\n",
      "Loss: 2.7046301458146838;\n",
      "Improved from 2.758474910736084 to 2.7431003170013426, saving model..\n",
      "Epoch: 11, Train loss: 2.706, Val loss: 2.743,            Epoch time=426.481s\n",
      "Loss: 2.616812429904938;\n",
      "Loss: 2.6304032731056215;\n",
      "Loss: 2.63990016857783;\n",
      "Loss: 2.6458950482606887;\n",
      "Loss: 2.650148960208893;\n",
      "Loss: 2.6552200389703113;\n",
      "Loss: 2.6609480043819973;\n",
      "Loss: 2.66415081679821;\n",
      "Loss: 2.6675258780055575;\n",
      "Improved from 2.7431003170013426 to 2.715742492675781, saving model..\n",
      "Epoch: 12, Train loss: 2.669, Val loss: 2.716,            Epoch time=439.365s\n",
      "Loss: 2.5881681365966798;\n",
      "Loss: 2.6027591624259947;\n",
      "Loss: 2.607272255738576;\n",
      "Loss: 2.610879966378212;\n",
      "Loss: 2.6172207816123962;\n",
      "Loss: 2.6225310672918956;\n",
      "Loss: 2.6268328004564556;\n",
      "Loss: 2.6307118253707884;\n",
      "Loss: 2.634959405051337;\n",
      "Improved from 2.715742492675781 to 2.698411769866943, saving model..\n",
      "Epoch: 13, Train loss: 2.637, Val loss: 2.698,            Epoch time=436.103s\n",
      "Loss: 2.548717391014099;\n",
      "Loss: 2.5615885899066924;\n",
      "Loss: 2.5734143878618876;\n",
      "Loss: 2.580549239039421;\n",
      "Loss: 2.5872149885177613;\n",
      "Loss: 2.59293910797437;\n",
      "Loss: 2.5988788784572057;\n",
      "Loss: 2.602764494776726;\n",
      "Loss: 2.607251971933577;\n",
      "Improved from 2.698411769866943 to 2.683592390060425, saving model..\n",
      "Epoch: 14, Train loss: 2.609, Val loss: 2.684,            Epoch time=432.433s\n",
      "Loss: 2.5281584186553956;\n",
      "Loss: 2.5379907252788545;\n",
      "Loss: 2.5480034818649293;\n",
      "Loss: 2.557258640050888;\n",
      "Loss: 2.5628958159446715;\n",
      "Loss: 2.569344180504481;\n",
      "Loss: 2.5736083488464354;\n",
      "Loss: 2.5778784449100494;\n",
      "Loss: 2.581875086519453;\n",
      "Improved from 2.683592390060425 to 2.6763608465194704, saving model..\n",
      "Epoch: 15, Train loss: 2.584, Val loss: 2.676,            Epoch time=435.569s\n",
      "Loss: 2.5075155334472656;\n",
      "Loss: 2.5154621715545655;\n",
      "Loss: 2.526700063228607;\n",
      "Loss: 2.5332919495105743;\n",
      "Loss: 2.541104629802704;\n",
      "Loss: 2.5463077418804168;\n",
      "Loss: 2.551899593830109;\n",
      "Loss: 2.5563111850619316;\n",
      "Loss: 2.560051528400845;\n",
      "Improved from 2.6763608465194704 to 2.661372254371643, saving model..\n",
      "Epoch: 16, Train loss: 2.562, Val loss: 2.661,            Epoch time=474.362s\n",
      "Loss: 2.4828494510650634;\n",
      "Loss: 2.4955773429870605;\n",
      "Loss: 2.5049171244303388;\n",
      "Loss: 2.5135537009239197;\n",
      "Loss: 2.521013613510132;\n",
      "Loss: 2.52842600218455;\n",
      "Loss: 2.53338308933803;\n",
      "Loss: 2.5369391247034074;\n",
      "Loss: 2.5409167292382984;\n",
      "Improved from 2.661372254371643 to 2.650231011390686, saving model..\n",
      "Epoch: 17, Train loss: 2.543, Val loss: 2.650,            Epoch time=480.498s\n",
      "Loss: 2.462661716938019;\n",
      "Loss: 2.4739397909641268;\n",
      "Loss: 2.48334250497818;\n",
      "Loss: 2.4935521055459975;\n",
      "Loss: 2.498973595714569;\n",
      "Loss: 2.505799922466278;\n",
      "Loss: 2.5122677173614503;\n",
      "Loss: 2.5183588328361513;\n",
      "Loss: 2.522618572394053;\n",
      "Improved from 2.650231011390686 to 2.6397762727737426, saving model..\n",
      "Epoch: 18, Train loss: 2.525, Val loss: 2.640,            Epoch time=487.302s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[63], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, NUM_EPOCHS\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m      7\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[1;32m----> 8\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_generator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[0;32m     10\u001B[0m     val_loss \u001B[38;5;241m=\u001B[39m evaluate(transformer, valid_generator, loss_fn)\n",
      "Cell \u001B[1;32mIn[38], line 34\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, iterator, optimizer, criterion, print_every)\u001B[0m\n\u001B[0;32m     32\u001B[0m texts_ru_out \u001B[38;5;241m=\u001B[39m texts_ru[\u001B[38;5;241m1\u001B[39m:, :]\n\u001B[0;32m     33\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(logits\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, logits\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]), texts_ru_out\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m---> 34\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     36\u001B[0m epoch_loss\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gpt2\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gpt2\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train(transformer, training_generator, optimizer, loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer, valid_generator, loss_fn)\n",
    "\n",
    "    if not losses:\n",
    "        print(f'First epoch - {val_loss}, saving model..')\n",
    "        torch.save(transformer, 'model')\n",
    "\n",
    "    elif val_loss < min(losses):\n",
    "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
    "        torch.save(transformer, 'model')\n",
    "\n",
    "    losses.append(val_loss)\n",
    "\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
    "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T02:39:30.290220700Z",
     "start_time": "2023-06-17T00:13:08.971318700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "\n",
    "\n",
    "    input_ids = [tokenizer_en.token_to_id('[CLS]')] + tokenizer_en.encode(text).ids[:max_len_en] + [tokenizer_en.token_to_id('[SEP]')]\n",
    "    output_ids = [tokenizer_ru.token_to_id('[CLS]')]\n",
    "\n",
    "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)]).to(DEVICE)\n",
    "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
    "\n",
    "    (texts_en_mask, texts_ru_mask,\n",
    "    texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
    "    logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
    "                   texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "    pred = logits.argmax(2).item()\n",
    "\n",
    "    while pred not in [tokenizer_ru.token_to_id('[SEP]'), tokenizer_ru.token_to_id('[PAD]')]:\n",
    "        output_ids.append(pred)\n",
    "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
    "\n",
    "        (texts_en_mask, texts_ru_mask,\n",
    "        texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
    "        logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
    "                       texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "        pred = logits.argmax(2)[-1].item()\n",
    "\n",
    "    return (' '.join([tokenizer_ru.id_to_token(i).replace('##', '') for i in output_ids[1:]]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T02:39:32.934868400Z",
     "start_time": "2023-06-17T02:39:32.922232900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "'Ты можешь это перевести ?'"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('Can you translate that?')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T02:39:33.616871400Z",
     "start_time": "2023-06-17T02:39:33.558777700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4548019047027907\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room']\n",
    "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room']\n",
    "\n",
    "\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, auto_reweigh=True)\n",
    "print(BLEUscore)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T02:39:37.805395600Z",
     "start_time": "2023-06-17T02:39:37.516390900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "en_test_sents = open('opus.en-ru-test.en.txt', encoding=\"UTF8\").read().lower().splitlines()\n",
    "ru_test_sents = open('opus.en-ru-test.ru.txt', encoding=\"UTF8\").read().lower().splitlines()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T02:39:43.944220800Z",
     "start_time": "2023-06-17T02:39:43.923971900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "translations = []\n",
    "\n",
    "for i in range(len(en_test_sents)):\n",
    "    translations.append(translate(en_test_sents[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T02:43:39.994546400Z",
     "start_time": "2023-06-17T02:39:45.372766400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bleus = []\n",
    "\n",
    "for i, t in enumerate(translations):\n",
    "    reference = tokenizer_ru.encode(t).tokens\n",
    "    hypothesis = tokenizer_ru.encode(ru_test_sents[i]).tokens\n",
    "\n",
    "    bleus.append(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  auto_reweigh=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "6.910610048631724"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(bleus)/len(bleus))*100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T02:43:40.305738600Z",
     "start_time": "2023-06-17T02:43:40.274534300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "b5aa93d6",
   "metadata": {},
   "source": [
    "\n",
    "## Задание 2 (2 балла).\n",
    "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/10.pdf \n",
    "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как его применить к паре en-ru на данных из семинара."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Backtranslation - один из методов, относящихся к технике Data augmentation (увеличения данных), которая применяется при недостатке обучающих данных (часто при работе с малоресурсными языками). Идея обратного перевода опирается на предположение, что существующие небольшие параллельные корпуса можно дополнить синтетическими битекстами, основанными на материалах одноязычного корпуса. Предположим, у нас есть небольшой параллельный корпус en-ru, где en - исходный язык, ru - целевой, а также много данных на ru. При обратном переводе модель сначала обучается на существующем параллельном корпусе в обратную сторону - перевод «ЦЯ-ИЯ». Эту модель используется для перевода одноязычных данных ru на en. Таким образом мы получаем синтетический параллельный корпус, данные которого можем перемешать с изначальным параллельным корпусом и затем обучить модель на всех имеющихся данных уже в направлении «ИЯ-ЦЯ»."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
