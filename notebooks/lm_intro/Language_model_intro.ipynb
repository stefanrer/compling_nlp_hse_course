{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Языковое моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Языковые модели - важнейшая часть современного NLP. Практически во всех задачах, связанных с обработкой текста, напрямую или косвенно используются языковые модели. А наиболее известные недавние прорывы в области - это по большей части новые подходы к языковому моделированию. ELMO, BERT, GPT - это языковые модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это достаточно сложная тема, поэтому будем разбирать постепенно. Сегодня разберём самые основы. Научимся приписывать вероятность последовательности слов и попробуем генерировать текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:21.771975100Z",
     "start_time": "2023-06-09T12:39:21.325116Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:21.780981400Z",
     "start_time": "2023-06-09T12:39:21.334083300Z"
    }
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем два разных корпуса: новостной и сообщения с 2ch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:21.852464800Z",
     "start_time": "2023-06-09T12:39:21.356842400Z"
    }
   },
   "outputs": [],
   "source": [
    "# !!! двач не самое приятное место, большое количество текстов в этом корпусе токсичные\n",
    "dvach = open('2ch_corpus.txt', encoding='UTF8').read()\n",
    "news = open('lenta.txt', encoding='UTF8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По длине они сопоставимы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:21.852464800Z",
     "start_time": "2023-06-09T12:39:21.495367900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина 1 - 11638405\n",
      "Длина 2 - 11536552\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина 1 -\", len(dvach))\n",
    "print(\"Длина 2 -\", len(news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем простую функцию для нормализации. Удалять пунктуацию и приводить к нижнему регистру, строго говоря не стоит, сгенерированный текст так будет не похож на настоящий. Но это немного упростит нам работу. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:21.855474Z",
     "start_time": "2023-06-09T12:39:21.501159Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним тексты по токенам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:39.376217600Z",
     "start_time": "2023-06-09T12:39:21.520485Z"
    }
   },
   "outputs": [],
   "source": [
    "norm_dvach = normalize(dvach)\n",
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:39.380247400Z",
     "start_time": "2023-06-09T12:39:39.377224500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина корпуса токсичных постов в токенах - 1858941\n",
      "Длина корпуса новостных текстов в токенах -  1505789\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина корпуса токсичных постов в токенах -\", len(norm_dvach))\n",
    "print(\"Длина корпуса новостных текстов в токенах - \", len(norm_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И по уникальным токенам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:39.887876700Z",
     "start_time": "2023-06-09T12:39:39.381262300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных токенов в корпусе токсичных постов - 189515\n",
      "Уникальный токенов в корпусе новостных текстов -  116302\n"
     ]
    }
   ],
   "source": [
    "print(\"Уникальных токенов в корпусе токсичных постов -\", len(set(norm_dvach)))\n",
    "print(\"Уникальный токенов в корпусе новостных текстов - \", len(set(norm_news)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем, сколько раз встречаются слова и выведем самые частотные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:39.889894700Z",
     "start_time": "2023-06-09T12:39:39.887311500Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:40.465139800Z",
     "start_time": "2023-06-09T12:39:39.889894700Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_dvach = Counter(norm_dvach)\n",
    "vocab_news = Counter(norm_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:40.512342500Z",
     "start_time": "2023-06-09T12:39:40.466649400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('и', 55892),\n ('в', 48853),\n ('не', 46602),\n ('на', 29660),\n ('что', 26668),\n ('я', 21734),\n ('а', 21310),\n ('с', 21080),\n ('это', 17727),\n ('ты', 15469)]"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dvach.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:40.530855Z",
     "start_time": "2023-06-09T12:39:40.512342500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('в', 72412),\n ('и', 33290),\n ('на', 28434),\n ('по', 19490),\n ('что', 17031),\n ('с', 15921),\n ('не', 12702),\n ('из', 7727),\n ('о', 7515),\n ('как', 7514)]"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_news.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы превратить абсолютные частоты в вероятности, разделим на общее число слов в каждом корпусе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:40.596512300Z",
     "start_time": "2023-06-09T12:39:40.531357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_news['ваываываываываыва']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:40.658490700Z",
     "start_time": "2023-06-09T12:39:40.569844200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('и', 0.030066580918921042),\n ('в', 0.02628001641794979),\n ('не', 0.02506911192985684),\n ('на', 0.015955320798239428),\n ('что', 0.014345802260534357),\n ('я', 0.011691602907246653),\n ('а', 0.011463516055646737),\n ('с', 0.011339789697467536),\n ('это', 0.009536074571489897),\n ('ты', 0.008321404498582796),\n ('как', 0.007882444897390503),\n ('у', 0.006848522895562581),\n ('но', 0.005786090037284669),\n ('так', 0.005383172462170666),\n ('по', 0.005060945990217011),\n ('то', 0.005049649235774562),\n ('все', 0.0046537248896011225),\n ('за', 0.004583792600195488),\n ('же', 0.004228751746289958),\n ('если', 0.004209385881531474)]"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_dvach = Counter({word:c/len(norm_dvach) for word, c in vocab_dvach.items()})\n",
    "probas_dvach.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:40.719187300Z",
     "start_time": "2023-06-09T12:39:40.663990100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('в', 0.04808907489694771),\n ('и', 0.0221080111489724),\n ('на', 0.018883123731146926),\n ('по', 0.012943380513471676),\n ('что', 0.011310349590812525),\n ('с', 0.01057319451795703),\n ('не', 0.008435444806676101),\n ('из', 0.005131529052211166),\n ('о', 0.00499073907433246),\n ('как', 0.0049900749706632205),\n ('к', 0.00407161959610543),\n ('за', 0.0040125143695431435),\n ('россии', 0.0036751497055696383),\n ('для', 0.003325831175549828),\n ('его', 0.003260084912295149),\n ('он', 0.0031704309169478593),\n ('от', 0.003066830744546547),\n ('сообщает', 0.003050228152815567),\n ('а', 0.0029180715226369697),\n ('также', 0.002716184007188258)]"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})\n",
    "probas_news.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти вероятности уже можно использовать, чтобы ответить на вопрос - это предложение больше подходит для новостей или для анонимного форума?\n",
    "\n",
    "В теории вероятностей для того, чтобы найти общую вероятность нескольких независимых событий произойти одновременно, нужно перемножить вероятности отдельных событий. В нашем случае мы хотим найти вероятность получить данное предложение. Для этого мы перемножаем вероятности слов в этом предложении. \n",
    "\n",
    "(Если бы мы сложили вероятности, то мы бы получили вероятность выбрать из корпуса 1 из слов в данном предложении)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем простую функцию, которая расчитает обшую вероятность. В прошлом семинаре мы говорили о том, что вместо умножения вероятностей можно складывать логарифмы от них. Еще нам нужно учесть одну деталь - некоторых слов может не быть в словаре и, соответственно, вероятность будет нулевая. Можно использовать в таких случаях небольшое значение вероятности, например 1/длина корпуса. Исправить это по-нормальному - сложно, придется подробнее разбираться с вероятностями, сглаживаниями и заменой неизвестных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:40.719187300Z",
     "start_time": "2023-06-09T12:39:40.714028700Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_joint_proba(text, word_probas):\n",
    "    prob = 0\n",
    "    for word in normalize(text):\n",
    "        if word in word_probas:\n",
    "            prob += (np.log(word_probas[word]))\n",
    "        else:\n",
    "            prob += (np.log(1/len(norm_dvach)))\n",
    "    \n",
    "    return np.exp(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:40.750987700Z",
     "start_time": "2023-06-09T12:39:40.717188500Z"
    }
   },
   "outputs": [],
   "source": [
    "phrase = 'Технические возможности устаревшего российского судна не позволили разгрузить его у терминала'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расчитаем вероятность встретить такой текст в каждом из корпусов (для таких маленьких чисел нужно смотреть на степень после e: чем больше степень, тем больше вероятность; но тут легко запутаться так как степень будет отрицательная и больше будет число, которое ближе к нулю (-5 больше -10 например)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:40.790553600Z",
     "start_time": "2023-06-09T12:39:40.727130700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "3.8958314050721132e-50"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_joint_proba(phrase, probas_dvach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:40.802661800Z",
     "start_time": "2023-06-09T12:39:40.742977900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "4.573351371331133e-45"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_joint_proba(phrase, probas_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно просто доверится функции больше/меньше, чтобы не запутаться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:40.818204700Z",
     "start_time": "2023-06-09T12:39:40.758520100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_joint_proba(phrase, probas_news) > compute_joint_proba(phrase, probas_dvach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вероятность встретить такой текст в новостном корпусе выше. Попробуем другой текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:40.818708700Z",
     "start_time": "2023-06-09T12:39:40.771759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'Безграмотное быдло с дубляжом, войсовером, порнографией и котикам'\n",
    "compute_joint_proba(phrase, probas_news) > compute_joint_proba(phrase, probas_dvach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут получается обратная ситуация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако такая оценка вероятности предложения черезчур упрощает действительность. Слова в предложении - это не независимые события, выбор первого слова сильно влияет на вероятности выбрать второе, третье и так далее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такие события можно оценивать по формуле полной вероятности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:40.818708700Z",
     "start_time": "2023-06-09T12:39:40.785038100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": "<img src=\"https://i.ibb.co/sC7CKzQ/image.png\" width=\"700\" height=\"500\"/>",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://i.ibb.co/sC7CKzQ/image.png\",\n",
    "     width=700, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А если простыми словами, то для того, чтобы получить вероятность предложения, нужно перемножить вероятность первого слова, вероятность второго слова, при условии первого, вероятность третьего при условии первого и второго, вероятность четвертого слова, при условии первого, второго и третьего и так далее до вероятности последнего слова при условии всех предшешевствующих.\n",
    "\n",
    "Условные вероятности для слов можно также вычислить по частотностям. Вероятность слова А при условии слова Б равна отношению количества раз, которое встретились слова А и Б вместе, к количеству раз, которое встретилось слово Б. Вероятность слова В при условии А и Б равна отношению количества раз, которое встретились слова А,Б и В вместе к количеству раз, которое встретились слова А и Б.\n",
    "И так далее. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но тут появляется проблема. Для того, чтобы расчитать полную вероятность предложения нужно, чтобы такое предложение уже встретилось в корпусе хотя бы 1 раз. Очевидно, что даже огромный корпус всего написанного текста не включает в себя все возможные тексты (тем более маленький корпус). Поэтому один из множителей в произведении будет нулевым, а значит и все произведение станет нулевым."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы этого избежать можно поубавить строгости и предположить, что вероятность слова зависит только от предыдущего слова. Это предположение называется марковским (в честь математика Андрея Маркова). Такую модель еще можно назвать биграммной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы расчитать вероятность с таким предположением, нам достаточно найти количество вхождений для каждого биграмма. А частоты отдельных слов у нас уже есть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:40.818708700Z",
     "start_time": "2023-06-09T12:39:40.802661800Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вероятность первого слово можно по идее считать просто как вероятность униграмма, но можно сделать небольшое добавление в нашу модель - поставить в начала каждого предложения технический токен начала предложения, а вероятность первого слова рассчитывать как вероятность биграма старт-первое слово поделить на частоту старта. Дальше мы будем генерировать текст с помощью языковой модели и это поможет нам генерировать более красивые предложения.\n",
    "\n",
    "\n",
    "Для того, чтобы у нас получились честные вероятности и можно было посчитать вероятность первого слова, нам нужно добавить тэг маркирующий начало предложений \\< start \\>\n",
    "\n",
    "Дальше мы попробуем сгенерировать текст, используя эти вероятности, и нам нужно будет когда-то остановится. Для этого добавим тэг окончания \\< end \\>\n",
    "\n",
    "Ну и поделим все на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\robst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:41.908462300Z",
     "start_time": "2023-06-09T12:39:40.809196Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:48.492593100Z",
     "start_time": "2023-06-09T12:39:41.909462Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences_dvach = [['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach[:5000000])]\n",
    "sentences_news = [['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news[:5000000])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:49.555372400Z",
     "start_time": "2023-06-09T12:39:48.493092500Z"
    }
   },
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence))\n",
    "\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы посчитать условную вероятность мы можем поделить количество вхождений на количество вхождений первого слова. Обновим нашу функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:49.558851Z",
     "start_time": "2023-06-09T12:39:49.556375700Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_joint_proba_markov_assumption(text, word_counts, bigram_counts):\n",
    "    prob = 0\n",
    "    for ngram in ngrammer(['<start>'] + normalize(phrase) + ['<end>']):\n",
    "        word1, word2 = ngram.split()\n",
    "        if word1 in word_counts and ngram in bigram_counts:\n",
    "            prob += np.log(bigram_counts[ngram]/word_counts[word1])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return np.exp(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:49.585303700Z",
     "start_time": "2023-06-09T12:39:49.558851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Эта фраза более вероятна в корпусе двача\n",
    "phrase = 'Безграмотное быдло с дубляжом, войсовером, порнографией и котикам'\n",
    "\n",
    "compute_joint_proba_markov_assumption(phrase, unigrams_dvach, bigrams_dvach) > \\\n",
    "compute_joint_proba_markov_assumption(phrase, unigrams_news, bigrams_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:49.618904800Z",
     "start_time": "2023-06-09T12:39:49.586307800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Эта фраза более вероятна в корпусе новостей\n",
    "phrase = 'Технические возможности устаревшего российского судна не позволили разгрузить его у терминала'\n",
    "\n",
    "compute_joint_proba_markov_assumption(phrase, unigrams_dvach, bigrams_dvach) > \\\n",
    "compute_joint_proba_markov_assumption(phrase, unigrams_news, bigrams_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем теперь генерировать текст с помощью биграмной языковой модели. Принцип генерации очень простой - на каждом шаге мы случайно выбираем следующее слово согласно вероятностям, расчитанным по 1 предыдущему слову."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В генерации мы можем выбирать только из уже известных слов. Можно заранее расчитать все вероятности и сохранить их в матрицу. Размерность матрицы слова на слова. В каждой ячееке будет лежать вероятность получить слово б, после слова а. Слово а будет в строке, а б в колонке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрицы получатся очень большими, но большинство значений будет нулевыми, поэтому можно воспользоваться разреженным форматом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:49.647762700Z",
     "start_time": "2023-06-09T12:39:49.609884Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:51.122386400Z",
     "start_time": "2023-06-09T12:39:49.628194400Z"
    }
   },
   "outputs": [],
   "source": [
    "# матрица слова на слова (инициализируем нулями)\n",
    "matrix_dvach = lil_matrix((len(unigrams_dvach), \n",
    "                         len(unigrams_dvach)))\n",
    "\n",
    "# к матрице нужно обращаться по индексам\n",
    "# поэтому зафиксируем порядок слов в словаре и сделаем маппинг id-слово и слово-id\n",
    "id2word_dvach = list(unigrams_dvach)\n",
    "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
    "\n",
    "# заполняем матрицу\n",
    "for ngram in bigrams_dvach:\n",
    "    word1, word2 = ngram.split()\n",
    "    # на пересечение двух слов ставим вероятность встретить второе после первого\n",
    "    matrix_dvach[word2id_dvach[word1], word2id_dvach[word2]] =  (bigrams_dvach[ngram]/\n",
    "                                                                     unigrams_dvach[word1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:52.272101400Z",
     "start_time": "2023-06-09T12:39:51.122887100Z"
    }
   },
   "outputs": [],
   "source": [
    "# то же самое для другого корпуса\n",
    "matrix_news = lil_matrix((len(unigrams_news), \n",
    "                        len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "\n",
    "\n",
    "for ngram in bigrams_news:\n",
    "    word1, word2 = ngram.split()\n",
    "    matrix_news[word2id_news[word1], word2id_news[word2]] =  (bigrams_news[ngram]/\n",
    "                                                                     unigrams_news[word1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для генерации нам понадобится функция np.random.choice , которая выбирает случайный объект из заданных. Ещё в неё можно подать вероятность каждого объекта и она будет доставать по ним (не только максимальный по вероятности)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:52.278794300Z",
     "start_time": "2023-06-09T12:39:52.276526100Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, word2id, n=100, start='<start>'):\n",
    "    text = []\n",
    "    current_idx = word2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])\n",
    "        # просто выбирать наиболее вероятное продолжение не получится\n",
    "        # можете попробовать раскоментировать следующую строчку и посмотреть что получается\n",
    "        # в современных языковых моделях есть специальный параметр, который\n",
    "        # позволяет регулировать разнообразность/случайность генерации\n",
    "        # он называется температура, чем выше температура тем ближе будет к argmax\n",
    "        # чем меньше температура тем ближе к полностью рандомной генерации\n",
    "        #         chosen = matrix[current_idx].argmax()\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = word2id['<start>']\n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:52.429520400Z",
     "start_time": "2023-06-09T12:39:52.278794300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "может и его исполнении когда бж были другие спермоворы это ты максималист которому просто взять себя теми чья это не такой пропуск это наглядная иллюстрация дискриминация \n",
      " и не упоминать \n",
      " мюридка просто орешь на трех крупнейших предприятий в dev ты меня расклад такой \n",
      " сперва пишут \n",
      " в час причем зачем тф 2 я знаю что они \n",
      " одна труба на стене висит над ней \n",
      " да и я ещё он сцука сц убейся нет \n",
      " просто напечатали еще и то же роковая женщина шашлык пожарил анонче что из социальных сетях \n",
      " такое « святое в кавычках \n",
      " какого\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2word_dvach, word2id_dvach).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:52.497672800Z",
     "start_time": "2023-06-09T12:39:52.432021500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "впрочем эта дата подписания в корпорации \n",
      " основополагающим при необходимости закон о которых обучают в городской избирательной кампании и сбиты с председателем ассоциации по утверждению минобороны поскольку если террористов \n",
      " такое решение судьи капитолины малышевой отклонил все прошли 11791 человек считаются крупнейшим падением спроса на себя также поддерживая ее с ужесточением борьбы с помощью планировалось сокращение выбросов в вашингтоне сообщает cnn в регистрации избирательных участках \n",
      " в районе и членами опек соглашения о нарушениях в документе обобщены и дагестана делегация которая уполномочена заявить на льготное налообложение через дунай поразил бы назвали в результате этих импровизированных производств работ продолжают поиски судна\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news, word2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:52.500305100Z",
     "start_time": "2023-06-09T12:39:52.498172900Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перплексия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерировать текст полностью требуется достаточно редко. И к тому же с этим более менее адекватно справляются только самые современные огромные модели. Гораздно более практичное применения языковой модели - выбрать наиболее вероятное продолжение уже введенной фразы. Вы все сталкивались с этим в своих телефонах. Если попытаться сгенировать текст только на основе предложенных слов, то получится не сильно лучше чем в текстах выше. Также языковую модель можно использовать, чтобы выбрать наиболее подходящее по контексту исправление опечатки и в этом случае совсем не важно, насколько красивые тексты она генерирует.\n",
    "\n",
    "Но как тогда оценивать качество языковой модели? Для этого стандартно используется перплексия (на русский обычно не переводят). У перплексии есть теоретическое обоснование в теории информации и даже какая-то интерпретация, но они достаточо сложные и непонятные. На практике можно просто считать, что перплексия показывает насколько хорошо языковая модель предсказывает корпус. Чем она ниже, тем лучше.\n",
    "\n",
    "Считается перплексия по вот такой формуле:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:52.504613Z",
     "start_time": "2023-06-09T12:39:52.501304800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": "<img src=\"https://i.ibb.co/Ph3sNMp/image.png\" width=\"500\" height=\"500\"/>",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://i.ibb.co/Ph3sNMp/image.png\",\n",
    "     width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простыми словами - нам нужно расчитать вероятность текста (мы это уже научились делать выше) и возвести ее в степень (-1/N), где N это количество слов в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:52.522292700Z",
     "start_time": "2023-06-09T12:39:52.505113100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Мы уже видели что произведение вероятностей можно заменить на экспоненту суммы логарифмов\n",
    "# С возведением в степень тоже есть удобное правило - log(x^y) = y * log(x)\n",
    "# можно заменить вот такую функцию (она ожидает вероятность)\n",
    "# def perplexity(p, N):\n",
    "#     return p**(-1/N) \n",
    "\n",
    "\n",
    "# на вот такую (результат должен совпадать)\n",
    "# функция ожидает логарифм вероятности\n",
    "def perplexity(logp, N):\n",
    "    return np.exp((-1/N) * logp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужно немного изменить функцию для расчета вероятности, чтобы возвращать N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:52.570082400Z",
     "start_time": "2023-06-09T12:39:52.522782700Z"
    }
   },
   "outputs": [],
   "source": [
    "# функции возвращают лог (чтобы проверить с первой функцией можно добавить np.exp(prob))\n",
    "def compute_joint_proba(text, word_probas):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    for word in tokens:\n",
    "        if word in word_probas:\n",
    "            prob += (np.log(word_probas[word]))\n",
    "        else:\n",
    "            prob += np.log(2e-4)\n",
    "    \n",
    "    return prob, len(tokens)\n",
    "\n",
    "\n",
    "def compute_join_proba_markov_assumption(text, word_counts, bigram_counts):\n",
    "    prob = 0\n",
    "    tokens = normalize(phrase)\n",
    "    for ngram in ngrammer(['<start>'] + tokens + ['<end>']):\n",
    "        word1, word2 = ngram.split()\n",
    "        if word1 in word_counts and ngram in bigram_counts:\n",
    "            prob += np.log(bigram_counts[ngram]/word_counts[word1])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return prob, len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть две функции для генераци вероятности последовательности. По сути каждая функция - это языковая модель. Первая - униграмная, а вторая биграмная.\n",
    "\n",
    "Таким образом мы можем сравнить две языковые модели расчитанные на одном корпусе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:52.582125900Z",
     "start_time": "2023-06-09T12:39:52.547496Z"
    }
   },
   "outputs": [],
   "source": [
    "phrase = 'Безграмотное быдло с дубляжом, войсовером, порнографией и котикам'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:52.592867600Z",
     "start_time": "2023-06-09T12:39:52.570082400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "17738.135055632363"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(*compute_joint_proba(phrase, probas_dvach))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:52.641566Z",
     "start_time": "2023-06-09T12:39:52.592867600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "233.89095512739362"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(*compute_join_proba_markov_assumption(phrase, unigrams_dvach, bigrams_dvach))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перплексия второй (биграмной модели) сильно меньше. Значит она лучше предсказывает корпус"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Со вторым текстом, который мы рассматривали до этого такое не сработает, потому что слишком много биграммов нет в словарях. Поэтому перплексия биграммной модели будет выше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:52.642074400Z",
     "start_time": "2023-06-09T12:39:52.614702700Z"
    }
   },
   "outputs": [],
   "source": [
    "phrase = 'Технические возможности устаревшего российского судна не позволили разгрузить его у терминала'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:52.642576200Z",
     "start_time": "2023-06-09T12:39:52.631545600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "10737.11899899257"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(*compute_joint_proba(phrase, probas_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:52.648781100Z",
     "start_time": "2023-06-09T12:39:52.636559200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "12287.628005974075"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(*compute_join_proba_markov_assumption(phrase, unigrams_news, bigrams_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но лучше оценивать качество языковой модели не на 1 тексте, а сразу на целом корпусе. Мы можем посчитать перплексию на всех предложениях и усреднить, чтобы получить общую перплексию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Униграмная модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:57.375410900Z",
     "start_time": "2023-06-09T12:39:52.647282500Z"
    }
   },
   "outputs": [],
   "source": [
    "ps = []\n",
    "for sent in sent_tokenize(dvach[:5000000]):\n",
    "    prob, N = compute_joint_proba(sent, probas_dvach)\n",
    "    if not N:\n",
    "        continue\n",
    "    ps.append(perplexity(prob, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:39:57.380925200Z",
     "start_time": "2023-06-09T12:39:57.375410900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "31346.173280655195"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бигрмная модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:40:01.287022600Z",
     "start_time": "2023-06-09T12:39:57.381932900Z"
    }
   },
   "outputs": [],
   "source": [
    "ps = []\n",
    "for sent in sent_tokenize(dvach[:5000000]):\n",
    "    prob, N = compute_join_proba_markov_assumption(sent, unigrams_dvach, bigrams_dvach)\n",
    "    if not N:\n",
    "        continue\n",
    "    ps.append(perplexity(prob, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:40:01.299347600Z",
     "start_time": "2023-06-09T12:40:01.291033300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "82659.58358943973"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T12:40:01.299848700Z",
     "start_time": "2023-06-09T12:40:01.294521100Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
