{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371970ff",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3. Исправление опечаток"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cf8bd",
   "metadata": {},
   "source": [
    "## 1. Доп. ранжирование по вероятности (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6be25c",
   "metadata": {},
   "source": [
    "Дополните get_closest_hybrid_match в семинаре так, чтобы из кандидатов с одинаковым расстоянием редактирования выбиралось наиболее вероятное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "import re\n",
    "import textdistance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import Counter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:47:45.857065300Z",
     "start_time": "2023-06-08T14:47:45.816556500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "corpus = open('data/wiki_data.txt', encoding='utf8').read()\n",
    "# создаем словарь\n",
    "vocab = Counter(re.findall('\\w+', corpus.lower()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:47:47.869372500Z",
     "start_time": "2023-06-08T14:47:45.820065100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "word2id = list(vocab.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,1))\n",
    "X = vec.fit_transform(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:47:49.054123400Z",
     "start_time": "2023-06-08T14:47:47.894294100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "def get_closest_match_vec(text, X, vec, topn=20):\n",
    "    v = vec.transform([text])\n",
    "\n",
    "    # вся эффективноть берется из того, что мы сразу считаем близость\n",
    "    # 1 вектора ко всей матрице (словам в словаре)\n",
    "    # считать по отдельности циклом было бы дольше\n",
    "    # вместо одного вектора может даже целая матрица\n",
    "    # тогда считаться в итоге будет ещё быстрее\n",
    "\n",
    "    similarities = cosine_distances(v, X)[0]\n",
    "    topn = similarities.argsort()[:topn]\n",
    "\n",
    "    return [(id2word[top], similarities[top]) for top in topn]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:47:49.058028300Z",
     "start_time": "2023-06-08T14:47:49.055123600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "def get_closest_match_with_metric(text, lookup,topn=20, metric=textdistance.levenshtein):\n",
    "    # Counter можно использовать и с не целыми числами\n",
    "    similarities = Counter()\n",
    "\n",
    "    for word in lookup:\n",
    "        similarities[word] = metric.normalized_similarity(text, word)\n",
    "\n",
    "    return similarities.most_common(topn)\n",
    "\n",
    "def get_closest_hybrid_match(text, X, vec, topn=3, metric=textdistance.damerau_levenshtein):\n",
    "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
    "    lookup = [cand[0] for cand in candidates]\n",
    "    closest = get_closest_match_with_metric(text, lookup, topn, metric=metric)\n",
    "\n",
    "    print(closest)\n",
    "\n",
    "    # Находим минимальное расстояние редактирования\n",
    "    min_distance = min([x[1] for x in closest])\n",
    "\n",
    "    print(min_distance)\n",
    "\n",
    "    # Выбираем кандидатов с минимальным расстоянием редактирования\n",
    "    closest_candidates = [x for x in closest if x[1] == min_distance]\n",
    "\n",
    "    # Выбираем наиболее вероятного кандидата\n",
    "    most_probable = max(closest_candidates, key=lambda x: P(x[0]))\n",
    "\n",
    "    return most_probable\n",
    "\n",
    "N = sum(vocab.values())\n",
    "\n",
    "def P(word, N=N):\n",
    "    return vocab[word] / N\n",
    "\n",
    "def predict_mistaken(word, vocab):\n",
    "    return 0 if word in vocab else 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:47:49.079324200Z",
     "start_time": "2023-06-08T14:47:49.059028200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e67f8d02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T14:47:49.182658500Z",
     "start_time": "2023-06-08T14:47:49.079324200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "[('кул', 1.0), ('кулу', 0.75), ('кулл', 0.75)]\n",
      "0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": "('кулу', 0.75)"
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "get_closest_hybrid_match('кул', X, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf9985",
   "metadata": {},
   "source": [
    "## 2.  Symspell (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392cc23",
   "metadata": {},
   "source": [
    "Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Там к словам применяется только одна операция - удаление символа. Описание алгоритма по шагам:\n",
    "\n",
    "1) Составляется словарь правильных слов  \n",
    "2) На основе словаря правильных слов составляется словарь удалений - для каждого правильного слова создаются все варианты удалений и создается словарь, где ключ - слово с удалением, а значение - правильное слово   \n",
    "3) Для выбора исправления для слова с опечаткой генерируются все варианты удаления, из них выбираются те, что есть в словаре удалений, построенного на шаге 2. Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления  \n",
    "4) Если в словаре удалений есть несколько вариантов, то выбирается удаление, которому соответствует наиболее вероятное правильное слово  \n",
    "\n",
    "\n",
    "Оцените качество полученного алгоритма теми же тремя метриками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:47:49.185629600Z",
     "start_time": "2023-06-08T14:47:49.183658500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "96c9fae0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T14:47:49.207668700Z",
     "start_time": "2023-06-08T14:47:49.194630900Z"
    }
   },
   "outputs": [],
   "source": [
    "class Symspell:\n",
    "    def __init__(self, dictionary):\n",
    "        self.dictionary = dictionary\n",
    "        self.deletion_dict = self.build_deletion_dict()\n",
    "        self.word_probabilities = self.build_probability_dict()\n",
    "\n",
    "    def build_deletion_dict(self):\n",
    "        deletion_dict = {}\n",
    "        for word in self.dictionary:\n",
    "            for i in range(len(word)):\n",
    "                deletion = word[:i] + word[i+1:]\n",
    "                if deletion not in deletion_dict:\n",
    "                    deletion_dict[deletion] = []\n",
    "                deletion_dict[deletion].append(word)\n",
    "        return deletion_dict\n",
    "\n",
    "    def build_probability_dict(self):\n",
    "        word_counts = Counter(self.dictionary)\n",
    "        total_words = sum(word_counts.values())\n",
    "        probability_dict = {word: count / total_words for word, count in word_counts.items()}\n",
    "        return probability_dict\n",
    "\n",
    "    def predict_mistaken(self, word):\n",
    "        return 0 if word in self.dictionary else 1\n",
    "\n",
    "    def correct(self, word):\n",
    "        if word in self.dictionary:\n",
    "            return word\n",
    "\n",
    "        candidates = []\n",
    "        for i in range(len(word)):\n",
    "            deletion = word[:i] + word[i+1:]\n",
    "            if deletion in self.deletion_dict:\n",
    "                candidates.extend(self.deletion_dict[deletion])\n",
    "\n",
    "        if not candidates:\n",
    "            return word\n",
    "\n",
    "        # выбираем кандидата с наибольшей вероятностью\n",
    "        best_candidate = max(candidates, key=lambda candidate: self.word_probabilities.get(candidate, 0))\n",
    "\n",
    "        return best_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "corpus = open('data/wiki_data.txt', encoding='utf8').read()\n",
    "# создаем словарь правильных слов\n",
    "vocab = Counter(re.findall('\\w+', corpus.lower()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:47:51.253549700Z",
     "start_time": "2023-06-08T14:47:49.207668700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "sym = Symspell(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:47:54.383896600Z",
     "start_time": "2023-06-08T14:47:51.253549700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": "'апофеоз'"
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "sym.correct('опофеоз')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:48:35.647708Z",
     "start_time": "2023-06-08T14:48:35.640369600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "bad = open('data/sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('data/correct_sents.txt', encoding='utf8').read().splitlines()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:47:54.404184900Z",
     "start_time": "2023-06-08T14:47:54.389474800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "\n",
    "    tokens_1 = [token.strip(punctuation) for token in tokens_1]\n",
    "    tokens_2 = [token.strip(punctuation) for token in tokens_2]\n",
    "\n",
    "    tokens_1 = [token for token in tokens_1 if token]\n",
    "    tokens_2 = [token for token in tokens_2 if token]\n",
    "\n",
    "    assert len(tokens_1) == len(tokens_2)\n",
    "\n",
    "    return list(zip(tokens_1, tokens_2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:47:54.436367500Z",
     "start_time": "2023-06-08T14:47:54.405184800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/915 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15838020b2324bf7853cca337aecadfa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8560280140070035\n",
      "0.19953416149068323\n",
      "0.04685884920179166\n"
     ]
    }
   ],
   "source": [
    "mistakes = []\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in tqdm(range(len(true))):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        if sym.predict_mistaken(pair[1]):\n",
    "            pred = cashed.get(pair[1], sym.correct(pair[1]))\n",
    "            cashed[pair[1]] = pred\n",
    "        else:\n",
    "            pred = pair[1]\n",
    "\n",
    "\n",
    "        if pred == pair[0]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes.append((pair[0], pair[1], pred))\n",
    "        total += 1\n",
    "\n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] != pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == pred:\n",
    "                mistaken_fixed += 1\n",
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:47:54.448025300Z",
     "start_time": "2023-06-08T14:47:54.412367300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "b292d96d",
   "metadata": {},
   "source": [
    "## *3. Чтение. (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee4b28f",
   "metadata": {},
   "source": [
    "Прочитайте эту главу в книге Speech and Language Processing - https://web.stanford.edu/~jurafsky/slp3/2.pdf .\n",
    "Ответьте на следующий вопрос:\n",
    "\n",
    "1. Что такое Byte-Pair Encoding (опишите по-русски, как минимум 10 предложениями)?\n",
    "\n",
    "*Это задание не связано напрямую с исправлением опечаток, но это важная тема, к которой мы вернемся в конце курса"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Byte-Pair Encoding (BPE) - это алгоритм сжатия текстов, который впоследствии был использован OpenAI для токенизации при предварительном обучении модели GPT.\n",
    "BPE отличается простотой и скоростью работы, что делает его подходящим для задач обработки естественного языка. Однако BPE имеет свои ограничения и недостатки.\n",
    "Например, он может не всегда корректно обрабатывать редкие и неизвестные слова. Кроме того, качество работы BPE зависит от качества исходного словаря правильных слов.\n",
    "Он используется многими моделями Transformer (включая GPT, GPT-2, RoBERTa, BART и DeBERTa).\n",
    "\n",
    "BPE начинается с составления словаря правильных слов. На основе этого словаря создается словарь удалений - для каждого правильного слова создаются все варианты удалений и создается словарь, где ключ - слово с удалением, а значение - правильное слово.\n",
    "Для выбора исправления для слова с опечаткой генерируются все варианты удаления, из них выбираются те, что есть в словаре удалений.\n",
    "Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления. Если в словаре удалений есть несколько вариантов, то выбирается удаление, которому соответствует наиболее вероятное правильное слово."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Или"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Byte-Pair Encoding (BPE) - это алгоритм сжатия текстов, который был предложен Sennrich и др. в 2016 году. Он начинается с составления словаря, который включает в себя все отдельные символы. Затем алгоритм анализирует обучающий корпус и выбирает два наиболее часто встречающихся рядом символа (например, ‘A’ и ‘B’), добавляет новый объединенный символ ‘AB’ в словарь и заменяет каждый смежный ‘A’ ‘B’ в корпусе на новый символ ‘AB’.\n",
    "\n",
    "Этот процесс повторяется, пока не будет выполнено k объединений, создавая k новых токенов; k является параметром алгоритма. Результирующий словарь состоит из исходного набора символов плюс k новых символов.\n",
    "\n",
    "Алгоритм обычно запускается внутри слов (не объединяя через границы слов), поэтому входной корпус сначала разделяется на пробелы, чтобы получить набор строк, каждая из которых соответствует символам слова, плюс специальный символ конца слова.\n",
    "\n",
    "После обучения словаря используется сегментатор токенов для токенизации тестового предложения. Сегментатор токенов просто запускает на тестовых данных объединения, которые мы изучили из обучающих данных, жадно, в порядке, в котором мы их изучили."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T14:47:54.448025300Z",
     "start_time": "2023-06-08T14:47:54.446239500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
